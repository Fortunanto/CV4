{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "\n",
    "\n",
    "## Homework 4 - Structure From Motion\n",
    "---\n",
    "\n",
    "### <a style='color:red'> Due Date: 26.01.2022 </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "#### READ THIS CAREFULLY\n",
    "* Submission only in **pairs**.\n",
    "* **No handwritten submissions**.\n",
    "* You can choose your working environment:\n",
    "    * You can work in a `Jupyter Notebook`, locally with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or online on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    * You can work in a Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both also allow opening/editing Jupyter Notebooks.\n",
    "* You should submit two **separated** files:\n",
    "    * A compressed `.zip` file, with the name: `ee046746_hw4_id1_id2.zip` which contains:\n",
    "        * A folder named `code` with all the code files inside (`.py` or `.ipynb` ONLY!), and all the files required for the code to run.\n",
    "            * **The code should run both on CPU and GPU without manual modifications**, require no special preparation and run on every computer.\n",
    "        * A folder named `output` with all the output files that are not required to run the code. This includes visualizations and saved data that are not included in your PDF report.\n",
    "    * A report file (visualizations, discussing the results and answering the questions) in a `.pdf` format, with the name `ee046746_hw4_id1_id2.pdf`.\n",
    "        * Be precise, we expect on point answers. **But don't be afraid to explain you statements (actually, we expect you to).**\n",
    "            * Even if the instructions says \"Show...\", you still need to explain what are you showing and what can be seen.\n",
    "    * No other file-types (`.docx`, `.html`, ...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/python.png\" style=\"height:50px;display:inline\"> Python Libraries\n",
    "---\n",
    "\n",
    "* `numpy`\n",
    "* `matplotlib`\n",
    "* `opencv` (or `scikit-image`)\n",
    "* `scikit-learn`\n",
    "* `scipy`\n",
    "* Anything else you need (`os`, `pandas`, `csv`, `json`,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for visualization windows to pop out in jupyter (kernel may require restart after using GUIs)\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Tasks\n",
    "---\n",
    "* In all tasks, you should document your process and results in a report file (which will be saved as `.pdf`). \n",
    "* You can reference your code in the report file, but no need for actual code in this file, the code is submitted in a seprate folder as explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/external-itim2101-lineal-color-itim2101/64/000000/external-robot-engineering-itim2101-lineal-color-itim2101.png\" style=\"height:50px;display:inline\"> Introduction \n",
    "---\n",
    "One of the major areas of computer vision is 3D reconstruction. Given several 2D images of an environment, can we recover the 3D structure of the environment, as well as the position of the camera/robot? This has many uses in robotics and autonomous systems, as understanding the 3D structure of the environment is crucial to navigation. You don't want your robot constantly bumping into walls, or running over human beings!\n",
    "\n",
    "<center> <img src=\"https://research.qut.edu.au/qcr/wp-content/uploads/sites/305/2021/02/Challenge_summary_pic-768x513.jpg\" style=\"height:300px\">\n",
    "<center> Image Source - <a href=\"https://research.qut.edu.au/qcr/2021/02/17/2nd-robotic-vision-scene-understanding-challenge-launched-cvpr2021-embodied-ai-workshop/\"> CVPR 21 Embodied AI Workshop </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 1, you will be writing a set of functions to generate a sparse point cloud for some test images we have provided to you. The test images are 2 renderings of a temple from two different angles. We have also provided you with a `npz` file containing good point correspondences between the two images. You will first write a function that computes the fundamental matrix between the two images. Then write a function that uses the epipolar constraint to find more point matches between the two images. Finally, you will write a function that will triangulate the 3D points for each pair of 2D point correspondences.\n",
    "\n",
    "In Part 2 <font color='red'>(Bonus)</font>, *if you wish to do it*, you will be writing a set of functions to calibrate a camera and project a 3D CAD model to a 2D image after estimating the camera pose. We have provided you with a `npz` file containing corresponding 2D-3D pairs. You will first write a function that estimates a camera matrix given 2D-3D calibration points. Then write a function to decompose the estimated camera matrix to intrinsic/extrinsic parameters. Finally, you will write a script to project the provided 3D CAD model and compare it to a given 2D image of an airplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/pastel-glyph/64/000000/pain-points.png\" style=\"height:50px;display:inline\"> Part 1 - Sparse Reconstruction \n",
    "---\n",
    "In this section, you will be writing a set of function to compute the sparse reconstruction from two sample images of a temple. You will first estimate the Fundamental matrix, compute point correspondences, then plot the results in 3D.\n",
    "It may be helpful to read through Section 1.5 right now. In Section 1.5 we ask you to write a testing script that will run your whole pipeline. It will be easier to start that now and add to it as you complete each of the questions one after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Eight Point Algorithm\n",
    "---\n",
    "In this question, you're going to use the eight point algorithm which is covered in class to estimate the fundamental matrix. Please use the point correspondences provided in `data/some corresp.npz`; you can load and view the contents of a `.npz` file as follows:\n",
    "> ``data = np.load(\"../data/some_corresp.npz\")``\n",
    "<br> `` print(data.files)`` </br>\n",
    "\n",
    "* Write the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scaling_matrix(scale_x,scale_y):\n",
    "    return np.array([[scale_x,0,0],[0,scale_y,0],[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eight_point(pts1, pts2, pmax):\n",
    "    \"\"\"\n",
    "    Eight Point Algorithm\n",
    "    [I] pts1, points in image 1 (Nx2 matrix)\n",
    "        pts2, points in image 2 (Nx2 matrix)\n",
    "        pmax, scalar value computed as max(H1,W1)\n",
    "    [O] F, the fundamental matrix (3x3 matrix)\n",
    "    \"\"\"\n",
    "    N =pts1.shape[0]\n",
    "    pts1_ones = np.column_stack((pts1,np.ones(N)))\n",
    "    pts2_ones = np.column_stack((pts2,np.ones(N)))\n",
    "    scaling_matrix = create_scaling_matrix(1/pmax,1/pmax)\n",
    "    \n",
    "    pts1_hat_s = (scaling_matrix@pts1_ones.T).T\n",
    "    pts2_hat_s = (scaling_matrix@pts2_ones.T).T\n",
    "    pts1_hat = np.repeat(pts1_hat_s,3).reshape(N,-1)\n",
    "    pts2_hat = np.repeat(pts2_hat_s,3,axis=0).reshape(N,-1)\n",
    "\n",
    "    pts1_hat_s = pts1_hat_s[:,:2]/np.expand_dims(pts1_hat_s[:,-1],axis=1)\n",
    "    pts2_hat_s = pts2_hat_s[:,:2]/np.expand_dims(pts2_hat_s[:,-1],axis=1)\n",
    "\n",
    "    row_mat = pts1_hat*pts2_hat\n",
    "    U,S,VT = np.linalg.svd(row_mat)\n",
    "    F_rank3 = VT[-1,:].reshape(3,3)\n",
    "    F = _singularize(F_rank3)\n",
    "    print(f\"shape of pts1_hat is {pts1_hat_s.shape} and of 2 is {pts2_hat_s.shape}\")\n",
    "    F = refineF(F,pts1_hat_s,pts2_hat_s)\n",
    "    F=scaling_matrix.T@F@scaling_matrix\n",
    "\n",
    "    # replace pass by your implementation\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `pts1` and `pts2` are $N \\times 2$ matrices corresponding to the $(x,y)$ coordinates of the $N$ points in the first and second image respectively, and `pmax` is a scale parameter. Implementation tips:\n",
    "* Normalize points and un-normalize $F$: You should scale the data by dividing each coordinate by $p_{\\text{max}}$ (the maximum of the image's width and height) using a transformation matrix $T$. After computing $F$, you will have to \"unscale\" the fundamental matrix. If $p_{\\text{norm}} = Tp$, then $F_{\\text{unnorm}} = T^T F T$. Note that this scaling is slightly simpler than \"centering\" that you did in the lecture, but for the purpose of this assingment it should suffice.\n",
    "\n",
    "* You must enforce the rank 2 constraint on $F$ before unscaling. Recall that a valid fundamental matrix $F$ will have all epipolar lines intersect at a certain point, meaning that there exists a non-trivial null space for $F$. In general, with real points, the eight-point solution for $F$ will not come with this condition. To enforce the rank 2 constraint, decompose $F$ with SVD to get the three matrices $U,\\Sigma,V$ such that $F = U\\Sigma V^T$. Then force the matrix to be rank 2 by setting the smallest singular value in $\\Sigma$ to zero, giving you a new $\\Sigma'$. Now compute the proper fundamental matrix with $F' = U\\Sigma' V^T$.\n",
    "\n",
    "* You may find it helpful to refine the solution by using local minimization. This probably won't fix a completely broken solution, but may make a good solution better by locally minimizing a geometric cost function. For this we have provided a helper function `refineF` taking in $F$ and the two sets of points, which you can call from `eight_point` before unscaling $F$.\n",
    "\n",
    "* Remember that the x-coordinate of a point in the image is its column entry and y-coordinate is the row entry. Also note that eight-point is just a \"figurative\" name, it just means that you need at least 8 points; your algorithm should use an over-determined system ($N > 8$ points).\n",
    "\n",
    "* To visualize the correctness of your estimated $F$, use the provided function `displayEpipolarF`, which takes in $F$, and the two images. This GUI lets you select a point in one of the images and visualize the corresponding epipolar line in the other image (Figure 1).\n",
    "\n",
    "*  Please include in your report the recovered $F$ and the visualization of some epipolar lines (similar to Figure 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function 1: singualrizes F using SVD\n",
    "def _singularize(F):\n",
    "    U, S, V = np.linalg.svd(F)\n",
    "    S[-1] = 0\n",
    "    F = U.dot(np.diag(S).dot(V))\n",
    "\n",
    "    return F\n",
    "\n",
    "# helper function 2.1: defines an objective function using F and the epipolar constraint\n",
    "def _objective_F(f, pts1, pts2):\n",
    "    F = _singularize(f.reshape([3, 3]))\n",
    "    num_points = pts1.shape[0]\n",
    "    hpts1 = np.concatenate([pts1, np.ones([num_points, 1])], axis=1)\n",
    "    hpts2 = np.concatenate([pts2, np.ones([num_points, 1])], axis=1)\n",
    "    Fp1 = F.dot(hpts1.T)\n",
    "    FTp2 = F.T.dot(hpts2.T)\n",
    "\n",
    "    r = 0\n",
    "    for fp1, fp2, hp2 in zip(Fp1.T, FTp2.T, hpts2):\n",
    "        r += (hp2.dot(fp1))**2 * (1/(fp1[0]**2 + fp1[1]**2) + 1/(fp2[0]**2 + fp2[1]**2))\n",
    "\n",
    "    return r\n",
    "\n",
    "# helper function 2.2: refines F using the objective from above and local optimization\n",
    "def refineF(F, pts1, pts2):\n",
    "    f = scipy.optimize.fmin_powell(\n",
    "        lambda x: _objective_F(x, pts1, pts2), F.reshape([-1]),\n",
    "        maxiter=100000,\n",
    "        maxfun=10000\n",
    "    )\n",
    "\n",
    "    return _singularize(f.reshape([3, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualization functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function 3.1: derives the epipoles using the essential matrix\n",
    "def _epipoles(E):\n",
    "    U, S, V = np.linalg.svd(E)\n",
    "    e1 = V[-1, :]\n",
    "    U, S, V = np.linalg.svd(E.T)\n",
    "    e2 = V[-1, :]\n",
    "\n",
    "    return e1, e2\n",
    "\n",
    "# helper function 3.2: GUI that uses F to draw the epipolar lines in I2 correponding to chosen pts in I1\n",
    "def displayEpipolarF(I1, I2, F):\n",
    "    e1, e2 = _epipoles(F)\n",
    "\n",
    "    sy, sx, _ = I2.shape\n",
    "\n",
    "    f, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 9))\n",
    "    ax1.imshow(I1)\n",
    "    ax1.set_title('Select a point in this image')\n",
    "    ax1.set_axis_off()\n",
    "    ax2.imshow(I2)\n",
    "    ax2.set_title('Verify that the corresponding point \\n is on the epipolar line in this image')\n",
    "    ax2.set_axis_off()\n",
    "\n",
    "    while True:\n",
    "        plt.sca(ax1)\n",
    "        x, y = plt.ginput(1, mouse_stop=2)[0]\n",
    "\n",
    "        xc, yc = int(x), int(y)\n",
    "        v = np.array([[xc], [yc], [1]])\n",
    "\n",
    "        l = F @ v\n",
    "        s = np.sqrt(l[0]**2+l[1]**2)\n",
    "\n",
    "        if s==0:\n",
    "            error('Zero line vector in displayEpipolar')\n",
    "\n",
    "        l = l / s\n",
    "        if l[1] != 0:\n",
    "            xs = 0\n",
    "            xe = sx - 1\n",
    "            ys = -(l[0] * xs + l[2]) / l[1]\n",
    "            ye = -(l[0] * xe + l[2]) / l[1]\n",
    "        else:\n",
    "            ys = 0\n",
    "            ye = sy - 1\n",
    "            xs = -(l[1] * ys + l[2]) / l[0]\n",
    "            xe = -(l[1] * ye + l[2]) / l[0]\n",
    "\n",
    "        ax1.plot(x, y, '*', MarkerSize=6, linewidth=2)\n",
    "        ax2.plot([xs, xe], [ys, ye], linewidth=2)\n",
    "        plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110, 2) (110, 2)\n",
      "shape of pts1_hat is (110, 2) and of 2 is (110, 2)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000106\n",
      "         Iterations: 74\n",
      "         Function evaluations: 7978\n",
      "F=[[-1.12821168e-09  1.23273785e-07 -6.24820288e-06]\n",
      " [ 6.41085705e-08  1.04660918e-10 -1.11138909e-03]\n",
      " [-1.31615517e-05  1.06851387e-03  4.47849325e-03]]\n"
     ]
    }
   ],
   "source": [
    "im1 = cv.cvtColor(cv.imread(\"data/im1.png\"),cv.COLOR_BGR2RGB)\n",
    "im2 = cv.cvtColor(cv.imread(\"data/im2.png\"),cv.COLOR_BGR2RGB)\n",
    "data = np.load(\"data/some_corresp.npz\")\n",
    "pts1 = data['pts1']\n",
    "pts2 = data['pts2']\n",
    "print(pts1.shape,pts2.shape)\n",
    "F = eight_point(pts1,pts2,max(im1.shape))\n",
    "# displayEpipolarF(im1,im2,F)\n",
    "print(f\"F={F}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example result:\n",
    "\n",
    "<center> <img src = \"assets/displayEpipolarF_example.jpg\" style=\"width:75%\">\n",
    "<center> Figure 1 - Epipolar lines visualization from \"displayEpipolarF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Epipolar Correpondences\n",
    "---\n",
    "To reconstruct a 3D scene with a pair of two images, we need to find many point pairs. A point pair is two points (one in each image) that correspond to the same 3D scene point. With enough of these pairs, when we plot the resulting 3D points, we will have a rough outline of the 3D object. You found point pairs in HW1 using feature detectors and feature descriptors, and testing a point in one image with every single point in the other image. But here we can use the fundamental matrix to greatly simplify this search.\n",
    "\n",
    "<center> <img src = \"assets/epipolar_theory.jpg\" style=\"width:50%\">\n",
    "<center> Figure 2 - Epipolar Geometry. Potential matches for $p$ lie on the epipolar line $l'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from class that given a point $p$ in one image (the left view in Figure 2). Its corresponding 3D scene point $P$ could lie anywhere along the line from the camera center $C$ to the point $p$. This line, along with a second image's camera center $C'$ (the right view in Figure 2) forms a plane. This plane intersects with the image plane of the second camera, resulting in a line $l'$ in the second image which describes all the possible locations that $p$ may be found in the second image. Line $l'$ is the epipolar line, and we only need to search along this line to find a match for point $p$ found in the first image.\n",
    "\n",
    "* Write the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def epipolar_correspondences(I1, I2, F, pts1):\n",
    "    \"\"\"\n",
    "    Epipolar Correspondences\n",
    "    [I] I1, image 1 (H1xW1 matrix)\n",
    "        I2, image 2 (H2xW2 matrix)\n",
    "        F, fundamental matrix from image 1 to image 2 (3x3 matrix)\n",
    "        pts1, points in image 1 (Nx2 matrix)\n",
    "    [O] pts2, points in image 2 (Nx2 matrix)\n",
    "    \"\"\"\n",
    "    N =pts1.shape[0]\n",
    "    pts1_ones = np.column_stack((pts1,np.ones(N)))\n",
    "    epipolar_lines_p2 = (F@(pts1_ones.T)).T\n",
    "    padding = 5\n",
    "    pts2 = []\n",
    "    for line_index,line in enumerate(epipolar_lines_p2):\n",
    "        x_pts1 = pts1[line_index][0]\n",
    "        y_pts1 = pts1[line_index][1]\n",
    "        pts1_line_window = I1[y_pts1-padding:y_pts1+padding+1,x_pts1-padding:x_pts1+padding+1].flatten()\n",
    "        possible_indices = [(x,int(np.floor(-(line[0]*x+line[2])/line[1]))) for x in range(padding,I2.shape[1]-padding)]\n",
    "        possible_indices = [(x,y) for (x,y) in possible_indices if padding<=y<=I2.shape[0]]\n",
    "        windows = [I2[y-padding:y+padding+1,x-padding:x+padding+1].flatten() for (x,y) in possible_indices]\n",
    "        correlations = distance.cdist(np.array(windows),[pts1_line_window],metric='cityblock')\n",
    "        min_indice = possible_indices[np.argmin(correlations)]\n",
    "        pts2 = pts2 + [np.array(min_indice)]\n",
    "    pts2_loc = np.array(pts2)\n",
    "    return pts2_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `I1` and `I2` are two-view images, `F` is the fundamental matrix computed for the two images using your `eight_point` function, `pts1` is a $N \\times 2$ matrix containing the $(x,y)$ points in the first image, and the function should return `pts2`, a $N \\times 2$ matrix, which contains the corresponding points in the second image. Implementation tips:\n",
    "\n",
    "* To match one point $p$ in image 1, use the fundamental matrix to estimate the corresponding epipolar line $l'$ and generate a set of candidate points in the second image.\n",
    "\n",
    "* For each candidate points $p'$, a similarity score between $p$ and $p'$ is computed. The point among candidates with highest score is treated as epipolar correspondence.\n",
    "\n",
    "* There are many ways to define the similarity between two points. Feel free to use whatever you want and **describe it in your write-up**. One possible solution is to select a small window of size $w$ around the point $p$. Then compare this target window to the window of the candidate point in the second image. For the images we gave you, simple Euclidean distance or Manhattan distance should suffice.\n",
    "\n",
    "* Remember to take care of data type and index range. You can use the provided function `epipolarMatchGUI` to visually test your function. Your function does not need to be perfect, but it should get most easy points correct, like corners, dots etc.\n",
    "\n",
    "* Please include a screenshot of `epipolarMatchGUI` running with your implementation of `epipolar_correspondences` (similar to Figure 3). Mention the similarity metric you decided to use. Also comment on any cases where your matching algorithm consistently fails, and why you might think this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function 4: GUI that uses F, and the matching function to draw the epipolar correpondences on the epipolar lines\n",
    "def epipolarMatchGUI(I1, I2, F):\n",
    "    e1, e2 = _epipoles(F)\n",
    "\n",
    "    sy, sx, sd = I2.shape\n",
    "\n",
    "    f, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 9))\n",
    "    ax1.imshow(I1)\n",
    "    ax1.set_title('Select a point in this image')\n",
    "    ax1.set_axis_off()\n",
    "    ax2.imshow(I2)\n",
    "    ax2.set_title('Verify that the corresponding point \\n is on the epipolar line in this image')\n",
    "    ax2.set_axis_off()\n",
    "\n",
    "    while True:\n",
    "        plt.sca(ax1)\n",
    "        x, y = plt.ginput(1, mouse_stop=2)[0]\n",
    "\n",
    "        xc, yc = int(x), int(y)\n",
    "        v = np.array([[xc], [yc], [1]])\n",
    "\n",
    "        l = F @ v\n",
    "        s = np.sqrt(l[0]**2+l[1]**2)\n",
    "\n",
    "        if s==0:\n",
    "            error('Zero line vector in displayEpipolar')\n",
    "\n",
    "        l = l / s\n",
    "        if l[0] != 0:\n",
    "            xs = 0\n",
    "            xe = sx - 1\n",
    "            ys = -(l[0] * xs + l[2]) / l[1]\n",
    "            ye = -(l[0] * xe + l[2]) / l[1]\n",
    "        else:\n",
    "            ys = 0\n",
    "            ye = sy - 1\n",
    "            xs = -(l[1] * ys + l[2]) / l[0]\n",
    "            xe = -(l[1] * ye + l[2]) / l[0]\n",
    "\n",
    "        ax1.plot(x, y, '*', MarkerSize=6, linewidth=2)\n",
    "        ax2.plot([xs, xe], [ys, ye], linewidth=2)\n",
    "\n",
    "        # draw points\n",
    "        pc = np.array([[xc, yc]])\n",
    "        p2 = epipolar_correspondences(I1, I2, F, pc)\n",
    "        ax2.plot(p2[0,0], p2[0,1], 'ro', MarkerSize=8, linewidth=2)\n",
    "        from matplotlib.patches import Rectangle\n",
    "        ax2.add_patch(Rectangle((p2[0]-14),29,29, linewidth=1, edgecolor='r', facecolor='none'))\n",
    "        ax1.add_patch(Rectangle((pc[0]-14),29,29, linewidth=1, edgecolor='r', facecolor='none'))\n",
    "        plt.show()\n",
    "        plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of pts1_hat is (110, 2) and of 2 is (110, 2)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000106\n",
      "         Iterations: 74\n",
      "         Function evaluations: 7978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiftachedelstain/miniconda3/envs/deep_learn/lib/python3.7/site-packages/ipykernel_launcher.py:40: MatplotlibDeprecationWarning: Case-insensitive properties were deprecated in 3.3 and support will be removed two minor releases later\n",
      "/Users/yiftachedelstain/miniconda3/envs/deep_learn/lib/python3.7/site-packages/ipykernel_launcher.py:46: MatplotlibDeprecationWarning: Case-insensitive properties were deprecated in 3.3 and support will be removed two minor releases later\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tt/54t4lpbs19vc76bfghvr0d640000gn/T/ipykernel_46235/1678519878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpts2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pts2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meight_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpts2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mepipolarMatchGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mim2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/tt/54t4lpbs19vc76bfghvr0d640000gn/T/ipykernel_46235/3393954860.py\u001b[0m in \u001b[0;36mepipolarMatchGUI\u001b[0;34m(I1, I2, F)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mginput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmouse_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mxc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "im1 = cv.cvtColor(cv.imread(\"data/im1.png\"),cv.COLOR_BGR2RGB)\n",
    "im2 = cv.cvtColor(cv.imread(\"data/im2.png\"),cv.COLOR_BGR2RGB)\n",
    "# print(im2.shape)\n",
    "data = np.load(\"data/some_corresp.npz\")\n",
    "pts1 = data['pts1']\n",
    "pts2 = data['pts2']\n",
    "F = eight_point(pts1,pts2,max(im1.shape))\n",
    "epipolarMatchGUI(im1,im2,F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example result:\n",
    "\n",
    "<center> <img src = \"assets/epipolarMatchGUI_example.jpg\" style=\"width:75%\">\n",
    "<center> Figure 3 - Epipolar Match visualization. A few errors are alright, but it should get most easy points correct (corners, dots, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Essential Matrix\n",
    "---\n",
    "In order to get the full camera projection matrices we need to compute the Essential matrix. So far, we have only been using the Fundamental matrix.\n",
    "\n",
    "* Write the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essential_matrix(F, K1, K2):\n",
    "    \"\"\"\n",
    "    Essential Matrix\n",
    "    [I] F, the fundamental matrix (3x3 matrix)\n",
    "        K1, camera matrix 1 (3x3 matrix)\n",
    "        K2, camera matrix 2 (3x3 matrix)\n",
    "    [O] E, the essential matrix (3x3 matrix)\n",
    "    \"\"\"\n",
    "    return K2.T@F@K1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of pts1_hat is (110, 2) and of 2 is (110, 2)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000106\n",
      "         Iterations: 74\n",
      "         Function evaluations: 7978\n",
      "E=[[-2.60799235e-03  2.85992513e-01  3.62513768e-02]\n",
      " [ 1.48730496e-01  2.43689428e-04 -1.66625526e+00]\n",
      " [ 3.53318183e-03  1.68735219e+00  1.91423949e-03]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"data/some_corresp.npz\")\n",
    "pts1 = data['pts1']\n",
    "pts2 = data['pts2']\n",
    "F = eight_point(pts1,pts2,max(im1.shape))\n",
    "intrinsics = np.load(\"data/intrinsics.npz\")\n",
    "K1 = intrinsics['K1']\n",
    "K2 = intrinsics['K2']\n",
    "\n",
    "E = essential_matrix(F,K1,K2)\n",
    "print(f\"E={E}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `F` is the Fundamental matrix computed between two images, `K1` and `K2` are the intrinsic camera matrices for the first and second image respectively (contained in `data/intrinsics.npz`), and `E` is the computed essential matrix. The intrinsic camera parameters are typically acquired through camera calibration. Refer to the class slides for the\n",
    "relationship between the Fundamental matrix and the Essential matrix.\n",
    "\n",
    "* Please include your estimated $E$ matrix for the temple image pair in the PDF report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 - Triangulation\n",
    "---\n",
    "Write a function to triangulate pairs of 2D points in the images to a set of 3D points.\n",
    "\n",
    "* Write the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate(M1, pts1, M2, pts2):\n",
    "    \"\"\"\n",
    "    Triangulation\n",
    "    [I] M1, camera projection matrix 1 (3x4 matrix)\n",
    "        pts1, points in image 1 (Nx2 matrix)\n",
    "        M2, camera projection matrix 2 (3x4 matrix)\n",
    "        pts2, points in image 2 (Nx2 matrix)\n",
    "    [O] pts3d, 3D points in space (Nx3 matrix)\n",
    "    \"\"\"\n",
    "    N = pts1.shape[0]\n",
    "    row1 =  np.expand_dims(pts1[:,1],axis=1)*M1[2,:]-M1[1,:]\n",
    "    row2 = M1[0,:]-np.expand_dims(pts1[:,0],axis=1)*M1[2,:]\n",
    "    row3 =  np.expand_dims(pts2[:,1],axis=1)*M2[2,:]-M2[1,:]\n",
    "    row4 = M2[0,:]-np.expand_dims(pts2[:,0],axis=1)*M2[2,:]\n",
    "    new_mat = np.vstack((row1.ravel(),row2.ravel(),row3.ravel(),row4.ravel()))\n",
    "    new_mat = new_mat.T.reshape(N,4,4).transpose(0,2,1)\n",
    "    _,_,VT = np.linalg.svd(new_mat)\n",
    "    return VT[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `pts1` and `pts2` are the $N\\times 2$ matrices with the 2D image coordinates, `M1` and `M2` are the $3\\times4$ camera projection matrices and `pts3d` is an $N\\times 3$ matrix with the corresponding 3D points (in all cases, one point per row). Remember that you will need to multiply the given intrinsic matrices with your solution for the extrinsic camera matrices to obtain the final camera projection matrices. \n",
    "\n",
    "* For `M1` you can assume no rotation or translation, so the extrinsic matrix is just $\\left[I \\lvert 0\\right]$. \n",
    "\n",
    "* For `M2`, pass the essential matrix to the provided function `camera2` to get four possible **extrinsic** matrices. You will need to determine which of these is the correct one to use. Refer to the class slides to remind yourself of the 4 possible camera layouts. The correct configuration is the one for which most of the 3D points are in front of both cameras (i.e. have a positive depth), and make up a reasonable recovered shape. To make sure you choose the right one, write a helper function that apply the extrinsics of camera 2 to the recovered 3D points and check that they are still in front of camera 1 (positive $Z$).\n",
    "\n",
    "* Keep in mind to multiply the extrinsics matrices by the corresponding intrinsics before inputting them to `triangulate`.\n",
    "* Once implemented, check the performance by looking at the re-projection error. To compute the re-projection error, project the estimated 3D points back to the image 1 and compute the mean Euclidean error between projected 2D points and the given `pts1`.\n",
    "\n",
    "* **In your write-up**: Describe how you determined which extrinsic matrix is correct. Note that simply rewording the hint is not enough. Report your re-projection error using the given `pts1` and `pts2` in `data/some_corresp.npz`. If implemented correctly, the re-projection error should be less than 2 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function 5: returns the 4 options for camera matrix M2 given the essential matrix\n",
    "def camera2(E):\n",
    "    U,S,V = np.linalg.svd(E)\n",
    "    m = S[:2].mean()\n",
    "    E = U.dot(np.array([[m,0,0], [0,m,0], [0,0,0]])).dot(V)\n",
    "    U,S,V = np.linalg.svd(E)\n",
    "    W = np.array([[0,-1,0], [1,0,0], [0,0,1]])\n",
    "\n",
    "    if np.linalg.det(U.dot(W).dot(V))<0:\n",
    "        W = -W\n",
    "\n",
    "    M2s = np.zeros([3,4,4])\n",
    "    M2s[:,:,0] = np.concatenate([U.dot(W).dot(V), U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
    "    M2s[:,:,1] = np.concatenate([U.dot(W).dot(V), -U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
    "    M2s[:,:,2] = np.concatenate([U.dot(W.T).dot(V), U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
    "    M2s[:,:,3] = np.concatenate([U.dot(W.T).dot(V), -U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
    "\n",
    "    return M2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of pts1_hat is (110, 2) and of 2 is (110, 2)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000106\n",
      "         Iterations: 74\n",
      "         Function evaluations: 7978\n",
      "reproj error camera 1 is 1.8849760499537738\n",
      "reproj error camera 2 is 1.8463951928975348\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"data/some_corresp.npz\")\n",
    "pts1 = data['pts1']\n",
    "pts2 = data['pts2']\n",
    "F = eight_point(pts1,pts2,max(im1.shape))\n",
    "intrinsics = np.load(\"data/intrinsics.npz\")\n",
    "K1 = intrinsics['K1']\n",
    "K2 = intrinsics['K2']\n",
    "\n",
    "E = essential_matrix(F,K1,K2)\n",
    "M2s = camera2(E)\n",
    "M1 = np.hstack((np.eye(3),np.expand_dims(np.zeros(3),axis=1)))\n",
    "\n",
    "index = -1\n",
    "for i in range(4):\n",
    "    M2 = M2s[:,:,i]\n",
    "    pts3d = triangulate(K1@M1,pts1,K2@M2,pts2)\n",
    "    if(pts3d[:,2]/pts3d[:,3]>0).all():\n",
    "        index = i\n",
    "            \n",
    "pts1_2d = (K1@M1@pts3d.T).T\n",
    "pts1_2d = pts1_2d[:,:2]/np.expand_dims(pts1_2d[:,-1],axis=1)\n",
    "reproj_err = np.diag(distance.cdist(pts1,pts1_2d))\n",
    "print(f\"reproj error camera 1 is {np.mean(reproj_err)}\")\n",
    "\n",
    "pts2_2d = (K2@M2s[:,:,index]@pts3d.T).T\n",
    "pts2_2d = pts2_2d[:,:2]/np.expand_dims(pts2_2d[:,-1],axis=1)\n",
    "\n",
    "reproj_err = np.diag(distance.cdist(pts2,pts2_2d))\n",
    "print(f\"reproj error camera 2 is {np.mean(reproj_err)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 - Putting It All Together\n",
    "---\n",
    "You now have all the pieces you need to generate a full 3D reconstruction. Write a test script `test_temple_coords` that does the following:\n",
    "\n",
    "* Load the two images and the point correspondences from `data/some_corresp.npz`.\n",
    "* Run eight point to compute the fundamental matrix `F`.\n",
    "* Load the points in image 1 contained in `data/temple_coords.npz` and run your `epipolar_correspondences` on them to get the corresponding points in image 2.\n",
    "* Load `data/intrinsics.npz` and compute the essential matrix `E`.\n",
    "* Compute the first camera projection matrix `M1` and use `camera2` to compute the four candidates for `M2`.\n",
    "* Run your `triangulate` function using the four sets of camera matrix candidates (after scaling with the intrinsics), the points from `data/temple_coords.npz`, and their computed correspondences.\n",
    "* Figure out the correct `M2` and the corresponding 3D points.\n",
    "* Use matplotlib's scatter function to plot these point correspondences on screen.\n",
    "* Report your computed extrinsic parameters (`R1`,`R2`,`t1`,`t2`) in your PDF.\n",
    "* We will use your test script to run your code, so be sure it runs smoothly. In particular, use relative paths to load files, not absolute paths.\n",
    "* **In your write-up**: Include 3 images of your final reconstruction of the points given in the file `data/temple_coords.npz`, from different angles as shown in Figure 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example result:\n",
    "\n",
    "<center> <img src = \"assets/pointcloud_example.jpg\" style=\"width:75%\">\n",
    "<center> Figure 4 - Sample Reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of pts1_hat is (110, 2) and of 2 is (110, 2)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000106\n",
      "         Iterations: 74\n",
      "         Function evaluations: 7978\n",
      "3 is now most likely as M2 255 points\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "im1 = cv.cvtColor(cv.imread(\"data/im1.png\"),cv.COLOR_BGR2RGB)\n",
    "im2 = cv.cvtColor(cv.imread(\"data/im2.png\"),cv.COLOR_BGR2RGB)\n",
    "data = np.load(\"data/some_corresp.npz\")\n",
    "pts1 = data['pts1']\n",
    "pts2 = data['pts2']\n",
    "#2\n",
    "F = eight_point(pts1,pts2,max(im1.shape))\n",
    "#3 \n",
    "temple_coords_pts1 = np.load(\"data/temple_coords.npz\")['pts1']\n",
    "temple_coords_pts2 =epipolar_correspondences(im1,im2,F,temple_coords_pts1)\n",
    "#4 \n",
    "intrinsics = np.load(\"data/intrinsics.npz\")\n",
    "K1 = intrinsics['K1']\n",
    "K2 = intrinsics['K2']\n",
    "E = essential_matrix(F,K1,K2)\n",
    "#5 \n",
    "M2s = camera2(E)\n",
    "M1 = np.hstack((np.eye(3),np.expand_dims(np.zeros(3),axis=1)))\n",
    "current_max_points = -np.inf\n",
    "M2_most_likely = None\n",
    "actual_pts3d = None\n",
    "for i in range(4):\n",
    "    M2 = M2s[:,:,i]\n",
    "    pts3d = triangulate(K1@M1,temple_coords_pts1,K2@M2,temple_coords_pts2)\n",
    "    c2_pts3d=(K2@M2@pts3d.T).T\n",
    "    if(pts3d[:,2]/pts3d[:,3]>0).all():\n",
    "        #if (c2_pts3d[:,2]>0).sum()>current_max_points:\n",
    "        print(i,\"is now most likely as M2\",(c2_pts3d[:,2]>0).sum(),\"points\")\n",
    "        M2_most_likely = M2\n",
    "        current_max_points = (c2_pts3d[:,2]>0).sum()\n",
    "        actual_pts3d=pts3d\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = plt.subplot(2,2,1,projection='3d')\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "pts3d = actual_pts3d[:,:3]/np.expand_dims(actual_pts3d[:,3],axis=1)\n",
    "ax.scatter(pts3d[:,0],pts3d[:,1],pts3d[:,2])\n",
    "ax = plt.subplot(2,2,2,projection='3d')\n",
    "pts3d = actual_pts3d[:,:3]/np.expand_dims(actual_pts3d[:,3],axis=1)\n",
    "ax.scatter(pts3d[:,0],pts3d[:,1],pts3d[:,2])\n",
    "ax = plt.subplot(2,2,3,projection='3d')\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "pts3d = actual_pts3d[:,:3]/np.expand_dims(actual_pts3d[:,3],axis=1)\n",
    "ax.scatter(pts3d[:,0],pts3d[:,1],pts3d[:,2])\n",
    "ax = plt.subplot(2,2,4,projection='3d')\n",
    "pts3d = actual_pts3d[:,:3]/np.expand_dims(actual_pts3d[:,3],axis=1)\n",
    "ax.scatter(pts3d[:,0],pts3d[:,1],pts3d[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1: [[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]] \n",
      "T1: [0 0 0] \n",
      "R2: [[ 5.11550341e-02 -3.37512488e-04 -1.92150423e-01 -3.43605170e-02]\n",
      " [ 4.11195943e-02  3.93130418e-04 -1.56474666e-01 -2.74361318e-02]\n",
      " [-2.54065903e-01  1.61965747e-03  9.67185553e-01  1.69495005e-01]] \n",
      "T2: [ 0.70935317 -0.00155716  0.06331687  0.70200188]\n"
     ]
    }
   ],
   "source": [
    "R2 = np.linalg.inv(K2)@M2_most_likely \n",
    "U,S,VT = np.linalg.svd(M2_most_likely,full_matrices=True)\n",
    "Translation = VT[-1,:]\n",
    "R1 = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0]])\n",
    "T1 = np.array([0,0,0])\n",
    "\n",
    "print(f\"R1: {R1} \\nT1: {T1} \\nR2: {R2} \\nT2: {Translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/external-ddara-lineal-ddara/64/000000/external-yoga-pose-yoga-poses-ddara-lineal-ddara-36.png\" style=\"height:50px;display:inline\"> Part 2 - Pose Estimation <font color='red'>(Bonus)</font>\n",
    "---\n",
    "In this section, you will implement what you have learned in class to estimate the intrinsic and extrinsic parameters of camera given 2D points $p$ on image and their corresponding 3D points $P$. In other words, given a set of matched points $\\left\\{P_i ,p_i\\right\\}$ and camera model (in homogeneous coordinates):\n",
    "\n",
    "$$ p = M P$$\n",
    "\n",
    "we want to find the estimate of the camera matrix $M\\in \\mathbb{R}^{3\\times4}$, as well as intrinsic parameter\n",
    "matrix $K\\in \\mathbb{R}^{3\\times3}$, camera rotation $R\\in \\mathbb{R}^{3\\times3}$ and camera translation $t\\in \\mathbb{R}^{3\\times1}$, such that:\n",
    "$$ M = K \\left[R \\lvert t\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Estimating $M$\n",
    "---\n",
    "* Write a function that estimates the camera matrix $M$ given 2D and 3D points $p,P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pose(p, P):\n",
    "    \"\"\"\n",
    "    Camera Matrix Estimation\n",
    "    [I] p, 2D points (Nx2 matrix)\n",
    "        P, 3D points (Nx3 matrix)\n",
    "    [O] M, camera matrix (3x4 matrix)\n",
    "    \"\"\"\n",
    "    A = []\n",
    "    N = p.shape[0]\n",
    "    P_ones = np.hstack((P,np.expand_dims(np.ones(N),axis=1)))\n",
    "    p_ones = np.hstack((p,np.expand_dims(np.ones(N),axis=1)))\n",
    "    for i in range(p.shape[0]):\n",
    "        A += [[*(P_ones[i,:]),*(4*[0]),*(-p_ones[i,0]*P_ones[i,:])],\n",
    "              [*(4*[0]),*(P_ones[i,:]),*(-p_ones[i,1]*P_ones[i,:])]]\n",
    "\n",
    "    A = np.array(A)\n",
    "    # print(A.shape)\n",
    "    _,_,VT = np.linalg.svd(A)\n",
    "    # replace pass by your implementation\n",
    "    return VT[-1,:].reshape(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `p` is $2\\times N$ matrix denoting the $(x, y)$ coordinates of the $N$ points on the image plane and `P` is $3\\times N$ matrix denoting the $(x,y,z)$ coordinates of the corresponding points in the 3D world. Recall that this camera matrix can be computed using the same strategy as homography estimation by Direct Linear Transform (DLT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once you finish this function, you can run the following script to test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojection Error with clean 2D points: 1.254846385795532e-10\n",
      "Pose Error with clean 2D points: 4.398703712906193e-12\n",
      "Reprojection Error with noisy 2D points: 5.345528132733343\n",
      "Pose Error with noisy 2D points: 1.8626490857163882\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate random camera matrix\n",
    "K = np.array([[1,0,100], [0,1,100], [0,0,1]])\n",
    "R, _,_ = la.svd(np.random.randn(3,3))\n",
    "if la.det(R) < 0: R = -R\n",
    "t = np.vstack((np.random.randn(2,1), 1))\n",
    "M = K @ np.hstack((R, t))\n",
    "\n",
    "# 2. Generate random 2D and 3D points\n",
    "N = 100\n",
    "P = np.random.randn(N,3)\n",
    "p = M @ np.hstack((P, np.ones((N,1)))).T\n",
    "p = p[:2,:].T / np.vstack((p[2,:], p[2,:])).T\n",
    "\n",
    "# 3. Test pose estimation with clean points\n",
    "Mc = estimate_pose(p, P)\n",
    "pp = Mc @ np.hstack((P, np.ones((N,1)))).T\n",
    "pp = pp[:2,:].T / np.vstack((pp[2,:], pp[2,:])).T\n",
    "\n",
    "print('Reprojection Error with clean 2D points:', la.norm(p-pp))\n",
    "print('Pose Error with clean 2D points:', la.norm((Mc/Mc[-1,-1])-(M/M[-1,-1])))\n",
    "\n",
    "# 4. Test pose estimation with noisy points\n",
    "p = p + np.random.rand(p.shape[0], p.shape[1])\n",
    "Mn = estimate_pose(p, P)\n",
    "pp = Mn @ np.hstack((P, np.ones((N,1)))).T\n",
    "pp = pp[:2,:].T / np.vstack((pp[2,:], pp[2,:])).T\n",
    "\n",
    "print('Reprojection Error with noisy 2D points:', la.norm(p-pp))\n",
    "print('Pose Error with noisy 2D points:', la.norm((Mn/Mn[-1,-1])-(M/M[-1,-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **In your write-up**: Please include the output of this script in your PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Intrinsic/Extrinsic Parameters\n",
    "---\n",
    "* Write a function that estimates both intrinsic and extrinsic parameters from a camera matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QR(M):\n",
    "    [R,Q]=np.linalg.qr(np.fliplr(np.flipud(M)).T);\n",
    "    nR=np.fliplr(np.flipud(R.T))\n",
    "    nQ=np.fliplr(np.flipud(Q.T))\n",
    "    return nQ,nR\n",
    "def estimate_params(M):\n",
    "    \"\"\"\n",
    "    Camera Parameter Estimation\n",
    "    [I] M, camera matrix (3x4 matrix)\n",
    "    [O] K, camera intrinsics (3x3 matrix)\n",
    "        R, camera extrinsics rotation (3x3 matrix)\n",
    "        t, camera extrinsics translation (3x1 matrix)\n",
    "    \"\"\"\n",
    "    _,_,VT = np.linalg.svd(M,full_matrices=True,)\n",
    "    camera_center = VT[-1,:]\n",
    "    camera_center = camera_center[:3]/camera_center[-1]\n",
    "    camera_center = np.expand_dims(camera_center,axis=1)\n",
    "    N= M[:,:3]\n",
    "    K,R=scipy.linalg.rq(N)\n",
    "    d = np.diag(np.sign(np.diag(K)))\n",
    "    K = K@d\n",
    "    R = d@R\n",
    "    translation = -R@camera_center\n",
    "    # replace pass by your implementation\n",
    "    return K,R,translation/translation[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we learned on class, the `estimate_params` function should consecutively run the following steps:\n",
    "* Compute the camera center $c$ by using SVD. Hint: $c$ is the eigenvector corresponding to the smallest eigenvalue.\n",
    "* Compute the intrinsic $K$ and rotation $R$ by using QR decomposition. $K$ is a right upper triangle matrix while $R$ is an orthonormal matrix.\n",
    "* Compute the translation by $t = -Rc$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once you finish this function, you can run the following script to test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our R\n",
      " [[-0.17451853  0.97595557 -0.13059096]\n",
      " [-0.9100081  -0.21051771 -0.357166  ]\n",
      " [ 0.37606986 -0.05650675 -0.92486672]]\n",
      "their R\n",
      " [[ 0.17451853 -0.97595557  0.13059096]\n",
      " [ 0.9100081   0.21051771  0.357166  ]\n",
      " [-0.37606986  0.05650675  0.92486672]]\n",
      "Intrinsic Error with clean 2D points: 2.7710880149728812e-12\n",
      "Rotation Error with clean 2D points: 3.4641016151377544\n",
      "Translation Error with clean 2D points: 3.818139530830896e-12\n",
      "Intrinsic Error with noisy 2D points: 0.6818343569364145\n",
      "Rotation Error with noisy 2D points: 3.464008677037615\n",
      "Translation Error with noisy 2D points: 0.06735047607849123\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate random camera matrix\n",
    "K = np.array([[1,0,100], [0,1,100], [0,0,1]])\n",
    "R, _,_ = la.svd(np.random.randn(3,3))\n",
    "if la.det(R) < 0: R = -R\n",
    "t = np.vstack((np.random.randn(2,1), 1))\n",
    "M = K @ np.hstack((R, t))\n",
    "\n",
    "# 2. Generate random 2D and 3D points\n",
    "N = 100\n",
    "P = np.random.randn(N,3)\n",
    "p = M @ np.hstack((P, np.ones((N,1)))).T\n",
    "p = p[:2,:].T / np.vstack((p[2,:], p[2,:])).T\n",
    "\n",
    "# 3. Test parameter estimation with clean points\n",
    "Mc = estimate_pose(p, P)\n",
    "Kc, Rc, tc = estimate_params(Mc)\n",
    "print(f\"our R\\n {Rc}\\ntheir R\\n {R}\")\n",
    "# print(f\"difference in theres is \\n{M[:,:3]-K@R}\")\n",
    "print('Intrinsic Error with clean 2D points:', la.norm((Kc/Kc[-1,-1])-(K/K[-1,-1])))\n",
    "print('Rotation Error with clean 2D points:', la.norm(R-Rc))\n",
    "print('Translation Error with clean 2D points:', la.norm(t-tc))\n",
    "\n",
    "# 4. Test parameter estimation with noisy points\n",
    "p = p + np.random.rand(p.shape[0], p.shape[1])\n",
    "Mn = estimate_pose(p, P)\n",
    "Kn, Rn, tn = estimate_params(Mn)\n",
    "\n",
    "print('Intrinsic Error with noisy 2D points:', la.norm((Kn/Kn[-1,-1])-(K/K[-1,-1])))\n",
    "print('Rotation Error with noisy 2D points:', la.norm(R-Rn))\n",
    "print('Translation Error with noisy 2D points:', la.norm(t-tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **In your write-up**: Please include the output of this script in your PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Projecting CAD Models to 2D\n",
    "---\n",
    "Now you will utilize what you have implemented to estimate the camera matrix from a real image, shown in Figure 5 (left), and project the 3D object (CAD model), shown in Figure 5 (right), back on to the image plane. \n",
    "\n",
    "<center> <img src = \"assets/airplane_image_CAD.jpg\" style=\"width:75%\">\n",
    "<center> Figure 5 - The provided image and 3D CAD model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a script that does the following:\n",
    "* Load an image `image`, a CAD model `cad`, 2D points `x` and 3D points `X` from the data file `data/pnp.npz`.\n",
    "* Run `estimate_pose` and `estimate_params` to estimate camera matrix $M$, intrinsic matrix $K$, rotation matrix $R$, and translation $t$.\n",
    "* Use your estimated camera matrix $M$ to project the given 3D points $P$ onto the image.\n",
    "* Plot the given 2D points $p$ and the projected 3D points on screen. An example is shown in Figure 6 (left).\n",
    "* Draw the CAD model rotated by your estimated rotation $R$ on screen. An example is shown in Figure 6 (middle).\n",
    "* Project the CAD's all vertices onto the image and draw the projected CAD model overlapping with the 2D image. An example is shown in Figure 6 (right).\n",
    "* **In your write-up**: Please include the three images similar to Figure 6 in your PDF. You must use different colors from Figure 6. For example, green circle for given 2D points, black points for projected 3D points, blue CAD model, and red projected CAD model overlapping on the image. You will get **NO credit** if you use the same color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example result:\n",
    "\n",
    "<center> <img src = \"assets/airplane_projection.jpg\" style=\"width:100%\">\n",
    "<center> Figure 6 - Project a CAD model back onto the image. Left: the image annotated with given 2D points (blue circle) and projected 3D points (red points); Middle: the CAD model rotated by estimated R; Right: the image overlapping with\n",
    "projected CAD model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "\n",
    "data = np.load(\"data/pnp.npz\")\n",
    "np.load = np_load_old\n",
    "\n",
    "p = data['x']\n",
    "P = data['X']\n",
    "N=p.shape[0]\n",
    "P_ones = np.hstack((P,np.expand_dims(np.ones(N),axis=1)))\n",
    "p_ones = np.hstack((p,np.expand_dims(np.ones(N),axis=1)))\n",
    "\n",
    "M = estimate_pose(p,P)\n",
    "K,R,t = estimate_params(M)\n",
    "p_es = (M@P_ones.T).T\n",
    "p_es = p_es[:,:2]/np.expand_dims(p_es[:,-1],axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.imshow(data['image'])\n",
    "plt.scatter(p_es[:,0],p_es[:,1],color='b')\n",
    "plt.scatter(p[:,0], p[:,1], s=160, facecolors='none', edgecolors='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.53043743]\n",
      " [-0.07053089]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cad = data['cad'][0][0][0]\n",
    "tr = data['cad'][0][0][1]-1\n",
    "# print(tr.shape)\n",
    "print(t)\n",
    "cad_rot = (R.T@cad.T).T\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot_trisurf(cad_rot[:,0], cad_rot[:,1], cad_rot[:,2], triangles = tr, edgecolor=[[1,0,0]], linewidth=1.0, alpha=0.0, shade=False)\n",
    "# ax.view_init(0,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(data['image'])\n",
    "N=cad.shape[0]\n",
    "cad_ones = np.hstack((cad,np.expand_dims(np.ones(N),axis=1)))\n",
    "cad_2d = (M@cad_ones.T).T\n",
    "cad_2d = cad_2d[:,:2]/np.expand_dims(cad_2d[:,-1],axis=1)\n",
    "plt.scatter(cad_2d[:,0],cad_2d[:,1],color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> References & Credits\n",
    "* Carnegie Mellon University - CMU\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
